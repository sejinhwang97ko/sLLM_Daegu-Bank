{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6448a3b7-a9c3-4e9e-a437-2227648369de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading base model from: NCSOFT/Llama-VARCO-8B-Instruct\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5616032b15b144b5abfa0b8888c67184",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading and merging PEFT from: ./llama3-8b-qa-ko/checkpoint-600\n",
      "Saving merged model to: ./output_dir\n",
      "✅ 모델과 토크나이저 저장 완료\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from peft import PeftModel\n",
    "\n",
    "# 경로 설정\n",
    "base_model_path = \"NCSOFT/Llama-VARCO-8B-Instruct\"\n",
    "adapter_path = \"./llama3-8b-qa-ko/checkpoint-600\"\n",
    "merged_model_path = \"./output_dir\"\n",
    "\n",
    "# 디바이스 설정\n",
    "device_arg = {\"device_map\": \"auto\"}\n",
    "\n",
    "# 베이스 모델 로드\n",
    "print(f\"Loading base model from: {base_model_path}\")\n",
    "base_model = AutoModelForCausalLM.from_pretrained(\n",
    "    base_model_path,\n",
    "    return_dict=True,\n",
    "    torch_dtype=torch.float16,\n",
    "    **device_arg\n",
    ")\n",
    "\n",
    "# LoRA 어댑터 로드 및 병합\n",
    "print(f\"Loading and merging PEFT from: {adapter_path}\")\n",
    "model = PeftModel.from_pretrained(base_model, adapter_path, **device_arg)\n",
    "model = model.merge_and_unload()\n",
    "\n",
    "# 토크나이저 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_path)\n",
    "\n",
    "# 저장\n",
    "print(f\"Saving merged model to: {merged_model_path}\")\n",
    "model.save_pretrained(merged_model_path)\n",
    "tokenizer.save_pretrained(merged_model_path)\n",
    "print(\"✅ 모델과 토크나이저 저장 완료\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccdf6f3-b1ed-4ff9-8246-a7a08eea63d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "from datasets import load_from_disk\n",
    "\n",
    "test_dataset = load_from_disk('test_dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "623a0773-a9fc-44d8-a112-2e3d700cc402",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'NCSOFT/Llama-VARCO-8B-Instruct'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "prompt_lst = []\n",
    "label_lst = []\n",
    "\n",
    "for messages in test_dataset[\"messages\"]:\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=False)\n",
    "    input = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[0] + '<|start_header_id|>assistant<|end_header_id|>\\n'\n",
    "    label = text.split('<|start_header_id|>assistant<|end_header_id|>\\n')[1].split('<|eot_id|>')[0]\n",
    "    prompt_lst.append(input)\n",
    "    label_lst.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "73ec39ab-9aca-4b8a-ab59-f92c511dafc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "eos_token = tokenizer(\"<|eot_id|>\",add_special_tokens=False)[\"input_ids\"][0]\n",
    "\n",
    "def test_inference(pipe, prompt):\n",
    "    outputs = pipe(prompt, max_new_tokens=1024, eos_token_id=eos_token, do_sample=False)\n",
    "    return outputs[0]['generated_text'][len(prompt):].strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dcfd61e-f5ec-40c4-9de7-ebf80794316a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "723396049c4b4d7d94a98e65eec7931c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model_id = './output_dir'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\", low_cpu_mem_usage=True, torch_dtype=torch.float16)\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "for prompt, label in zip(prompt_lst[300:305], label_lst[300:305]):\n",
    "    # print(f\"    prompt:\\n{prompt}\")\n",
    "    print(f\"    response:\\n{test_inference(pipe, prompt)}\")\n",
    "    print(f\"    label:\\n{label}\")\n",
    "    print(\"-\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c9e508e0-e233-4d56-9a99-ce77dcd06ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import HfApi\n",
    "api = HfApi()\n",
    "\n",
    "username = \"julietz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92776173-b3a0-40bf-a397-2a73f568c8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_NAME = 'llama-3-8b-rag-ko-checkpoint-600'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7511453b-0bdb-4f99-952f-aef18b124e04",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b961247484d4d2086bfdd61cd68d97b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9abf17df90e4aad8a2edaae0b335d42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 5 LFS files:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4be3a59e7eb84bce83e378d06e77bb79",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93ffe2768b7f4184929b283f747cf6b3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99b32625067e4471956541c3ac5fb1cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e07b8a12ef9a4ef3b375d703e41676f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/17.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/julietz/llama-3-8b-rag-ko-checkpoint-600/commit/3749adc55e3961910a26948d0765ec28e29b72a3', commit_message='Upload folder using huggingface_hub', commit_description='', oid='3749adc55e3961910a26948d0765ec28e29b72a3', pr_url=None, repo_url=RepoUrl('https://huggingface.co/julietz/llama-3-8b-rag-ko-checkpoint-600', endpoint='https://huggingface.co', repo_type='model', repo_id='julietz/llama-3-8b-rag-ko-checkpoint-600'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "api.create_repo(\n",
    "    token=\"key\",\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    repo_type=\"model\"\n",
    ")\n",
    "\n",
    "api.upload_folder(\n",
    "    token=\"key\",\n",
    "    repo_id=f\"{username}/{MODEL_NAME}\",\n",
    "    folder_path=\"output_dir\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52144cde-3328-4cb8-87e2-70b283c4ccf2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
